{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343c24aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakhat\\anaconda3\\envs\\irenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "requests.packages.urllib3.disable_warnings() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71239a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakhat\\anaconda3\\envs\\irenv\\lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:394: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch([{'host' : 'localhost', 'port' : 9200, 'scheme' : 'https'}], basic_auth=(\"elastic\",\"X8w8*Kabqp+5d5ROVoYM\"), verify_certs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a315874",
   "metadata": {},
   "source": [
    "# Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6568ac66",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'resource_already_exists_exception', 'index [en1kindex/qS1Vyc4pSgma5f_AEPL2Vw] already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men1kindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\irenv\\lib\\site-packages\\elasticsearch\\_sync\\client\\utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\irenv\\lib\\site-packages\\elasticsearch\\_sync\\client\\indices.py:517\u001b[0m, in \u001b[0;36mIndicesClient.create\u001b[1;34m(self, index, aliases, error_trace, filter_path, human, mappings, master_timeout, pretty, settings, timeout, wait_for_active_shards)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     __headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__body\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\irenv\\lib\\site-packages\\elasticsearch\\_sync\\client\\_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[1;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    380\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\irenv\\lib\\site-packages\\elasticsearch\\_sync\\client\\_base.py:320\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[1;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[0;32m    321\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: BadRequestError(400, 'resource_already_exists_exception', 'index [en1kindex/qS1Vyc4pSgma5f_AEPL2Vw] already exists')"
     ]
    }
   ],
   "source": [
    "es.indices.create(index='en1kindex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbcce8",
   "metadata": {},
   "source": [
    "# Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88743c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    'properties' : {\n",
    "        'text_right' : {\n",
    "            'type' : 'text'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c10959",
   "metadata": {},
   "outputs": [],
   "source": [
    "settingsNoStemmer = {\n",
    "    'analysis' : {\n",
    "        'analyzer' : {\n",
    "            'default' : {\n",
    "                'tokenizer' : 'whitespace'\n",
    "            },\n",
    "            \"default_search\": {\n",
    "                'tokenizer' : 'whitespace'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987b038",
   "metadata": {},
   "source": [
    "# Index modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976f4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index():\n",
    "    es.indices.delete(index='en1kindex')\n",
    "    es.indices.create(index='en1kindex', mappings=mappings, settings=settingsNoStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2cee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797ef7e",
   "metadata": {},
   "source": [
    "# Check Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4e4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_analyzer(analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    \n",
    "    tokens = es.indices.analyze(index='en1kindex', body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "207513a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'it was used in landing craft during world war ii and is used today in private boats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd59f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakhat\\AppData\\Local\\Temp\\ipykernel_11812\\1015729720.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index='en1kindex', body=body)['tokens']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'used',\n",
       " 'in',\n",
       " 'landing',\n",
       " 'craft',\n",
       " 'during',\n",
       " 'world',\n",
       " 'war',\n",
       " 'ii',\n",
       " 'and',\n",
       " 'is',\n",
       " 'used',\n",
       " 'today',\n",
       " 'in',\n",
       " 'private',\n",
       " 'boats']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'default'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24eb6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e4904",
   "metadata": {},
   "source": [
    "# Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67514d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_actions_generator():\n",
    "    with open('documents.csv', 'r') as en1k:\n",
    "        documentReader = csv.DictReader(en1k)\n",
    "        for document in documentReader:\n",
    "            text_right = document['text_right']\n",
    "            docID = document[\"id_right\"]\n",
    "            doc = json.dumps({'text_right' : text_right})\n",
    "            yield create_es_action('en1kindex', docID, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25dedcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 19.133228302001953 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for ok, result in parallel_bulk(es, es_actions_generator()):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed079f",
   "metadata": {},
   "source": [
    "# Perform Search and normalize Elasticsearch scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1cf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    res = es.search(index='en1kindex', query=query, size=50)['hits']\n",
    "    doc_content = []\n",
    "    scores = []\n",
    "    docInfo = {}\n",
    "    \n",
    "    for hit in res['hits']:\n",
    "        doc_content.append(hit[\"_source\"][\"text_right\"])\n",
    "        scores.append(hit[\"_score\"])\n",
    "        docInfo[hit[\"_source\"][\"text_right\"]] = hit[\"_id\"]\n",
    "        \n",
    "    scaler = MinMaxScaler()\n",
    "    scores = np.array(scores).reshape(-1, 1)\n",
    "    normalized_scores = scaler.fit_transform(scores).tolist()\n",
    "    \n",
    "    for i in range(0, 50):\n",
    "        normalized_scores[i].append(doc_content[i])\n",
    "        \n",
    "    return normalized_scores, docInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f995f",
   "metadata": {},
   "source": [
    "# Create query format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7087dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'bool': {\n",
    "        'should': [\n",
    "            {\n",
    "                'match': {\n",
    "                    'text_right': ''\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"match_all\": {}\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0029ff2",
   "metadata": {},
   "source": [
    "# Sampling queries from training dataset and running search on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f2db814",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryToDocs = {}\n",
    "docInfo = {}\n",
    "queryInfo = {}\n",
    "\n",
    "with open('training/queries.csv', 'r') as qw:\n",
    "    queryReader = csv.DictReader(qw)\n",
    "    max_size = 500\n",
    "    current_size = 0\n",
    "    \n",
    "    for q in queryReader:\n",
    "        if current_size >= max_size:\n",
    "            break\n",
    "        \n",
    "        if bool(random.getrandbits(1)):\n",
    "            current_size += 1\n",
    "            text_left = q['text_left']\n",
    "            queryInfo[text_left] = q[\"id_left\"]\n",
    "            query['bool']['should'][0]['match']['text_right'] = text_left\n",
    "            queryToDocs[text_left], partialDocInfo = search(query)\n",
    "            docInfo.update(partialDocInfo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1221f",
   "metadata": {},
   "source": [
    "# Generating runs corresponding to alpha values from 0 to 1 with step = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb529229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating run with alpha=0\n",
      "Finished generating run with alpha=0.05\n",
      "Finished generating run with alpha=0.1\n",
      "Finished generating run with alpha=0.15\n",
      "Finished generating run with alpha=0.2\n",
      "Finished generating run with alpha=0.25\n",
      "Finished generating run with alpha=0.3\n",
      "Finished generating run with alpha=0.35\n",
      "Finished generating run with alpha=0.4\n",
      "Finished generating run with alpha=0.45\n",
      "Finished generating run with alpha=0.5\n",
      "Finished generating run with alpha=0.55\n",
      "Finished generating run with alpha=0.6\n",
      "Finished generating run with alpha=0.65\n",
      "Finished generating run with alpha=0.7\n",
      "Finished generating run with alpha=0.75\n",
      "Finished generating run with alpha=0.8\n",
      "Finished generating run with alpha=0.85\n",
      "Finished generating run with alpha=0.9\n",
      "Finished generating run with alpha=0.95\n",
      "Finished generating run with alpha=1.0\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L6-cos-v5', device='cuda')\n",
    "\n",
    "alpha = 0\n",
    "\n",
    "while alpha <= 1:\n",
    "    with open(\"alpha_training/ST_\" + str(alpha) + \".res\", \"w\") as run:\n",
    "        for qry, doc_pairs in queryToDocs.items():\n",
    "            docs = []\n",
    "            es_scores = []\n",
    "\n",
    "            for doc in doc_pairs:\n",
    "                docs.append(doc[1])\n",
    "                es_scores.append(doc[0])\n",
    "                \n",
    "            query_embedding = model.encode(qry, convert_to_tensor=True)\n",
    "            doc_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "            cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "            result_scores = []\n",
    "            \n",
    "            for i in range(0, 50):\n",
    "                result_score = alpha * es_scores[i] + (1 - alpha) * cos_scores[i]\n",
    "                result_scores.append(result_score)\n",
    "                \n",
    "            top_results = torch.topk(torch.tensor(result_scores), k=50)\n",
    "            \n",
    "            rank = 0\n",
    "            for score, idx in zip(top_results[0], top_results[1]):\n",
    "                run.write(str(queryInfo[qry]) + \" Q0 \" + str(docInfo[docs[idx]]) + \" \" + str(rank) + \" \" + str(float(score)) + \" ST\\n\")\n",
    "                rank += 1\n",
    "                \n",
    "    print(\"Finished generating run with alpha=\" + str(alpha))\n",
    "    alpha = round(alpha + 0.05, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1d082",
   "metadata": {},
   "source": [
    "# Finding best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ed6f8",
   "metadata": {},
   "source": [
    "## Creators training run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "352c8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@20: 0.10813711911357335,\n",
       " P@10: 0.14584487534625967,\n",
       " AP@20: 0.10369050070071936}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('training/qrels')\n",
    "run = ir_measures.read_trec_run('training/BM25'  + '.res')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP@20], qrels, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32a377",
   "metadata": {},
   "source": [
    "## Searching for the best alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad99fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha value: 0.25\n",
      "With following precision metrics: {P@20: 0.058344875346260316, P@10: 0.08483379501385031, AP@20: 0.06224365087344461}\n",
      "Other alpha values and their corresponding precision metrics:\n",
      "(0, {P@20: 0.05519390581717443, P@10: 0.07873961218836552, AP@20: 0.0570357183876199})\n",
      "(0.05, {P@20: 0.05599030470914118, P@10: 0.08005540166204977, AP@20: 0.058944251653649214})\n",
      "(0.1, {P@20: 0.0569598337950138, P@10: 0.08199445983379491, AP@20: 0.06024241231715039})\n",
      "(0.15, {P@20: 0.057479224376731246, P@10: 0.0829639889196675, AP@20: 0.06102436520136578})\n",
      "(0.2, {P@20: 0.058240997229916845, P@10: 0.08379501385041545, AP@20: 0.06171702093026178})\n",
      "(0.25, {P@20: 0.058344875346260316, P@10: 0.08483379501385031, AP@20: 0.06224365087344461})\n",
      "(0.3, {P@20: 0.05865650969529078, P@10: 0.08462603878116332, AP@20: 0.06210492415624602})\n",
      "(0.35, {P@20: 0.058448753462603815, P@10: 0.08504155124653731, AP@20: 0.062033002131624375})\n",
      "(0.4, {P@20: 0.05858725761772843, P@10: 0.08531855955678666, AP@20: 0.061790104065910165})\n",
      "(0.45, {P@20: 0.05862188365650961, P@10: 0.08448753462603871, AP@20: 0.061776615064554743})\n",
      "(0.5, {P@20: 0.05841412742382264, P@10: 0.08448753462603871, AP@20: 0.06139148018524679})\n",
      "(0.55, {P@20: 0.05844875346260378, P@10: 0.08407202216066477, AP@20: 0.06110391273902709})\n",
      "(0.6, {P@20: 0.05824099722991682, P@10: 0.08296398891966751, AP@20: 0.060581043935795616})\n",
      "(0.65, {P@20: 0.058275623268697986, P@10: 0.0824792243767312, AP@20: 0.059823172164397885})\n",
      "(0.7, {P@20: 0.05834487534626029, P@10: 0.08240997229916887, AP@20: 0.05976804394766213})\n",
      "(0.75, {P@20: 0.05806786703601098, P@10: 0.08192520775623256, AP@20: 0.059162767781988645})\n",
      "(0.8, {P@20: 0.05792936288088634, P@10: 0.08178670360110789, AP@20: 0.058746346585471405})\n",
      "(0.85, {P@20: 0.057860110803324, P@10: 0.08116343490304696, AP@20: 0.05844920990306182})\n",
      "(0.9, {P@20: 0.057756232686980526, P@10: 0.08095567867035999, AP@20: 0.058136920020712156})\n",
      "(0.95, {P@20: 0.05737534626038774, P@10: 0.08047091412742369, AP@20: 0.05766114453437422})\n",
      "(1.0, {P@20: 0.056301939058171685, P@10: 0.07887811634349019, AP@20: 0.056202656284865884})\n"
     ]
    }
   ],
   "source": [
    "map20_max = 0\n",
    "alpha_best = -1\n",
    "alpha = 0\n",
    "res_best = {}\n",
    "all_res = []\n",
    "while alpha <=1:\n",
    "    qrels = ir_measures.read_trec_qrels('training/qrels')\n",
    "    run = ir_measures.read_trec_run('alpha_training/ST_' + str(alpha) + '.res')\n",
    "    res = ir_measures.calc_aggregate([P@10, P@20, MAP@20], qrels, run)\n",
    "    all_res.append((alpha, res))\n",
    "    \n",
    "    if res[MAP@20] > map20_max:\n",
    "        map20_max = res[MAP@20]\n",
    "        alpha_best = alpha\n",
    "        res_best = res\n",
    "        \n",
    "    alpha = round(alpha + 0.05, 2)\n",
    "    \n",
    "print(\"The best alpha value: \" + str(alpha_best))\n",
    "print(\"With following precision metrics: \" + str(res_best))\n",
    "print(\"Other alpha values and their corresponding precision metrics:\")\n",
    "for result in all_res:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b181c9a",
   "metadata": {},
   "source": [
    "# Applying the formula to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ccf97",
   "metadata": {},
   "source": [
    "## Performing search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7856c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryToDocs = {}\n",
    "docInfo = {}\n",
    "queryInfo = {}\n",
    "\n",
    "with open('test/queries.csv', 'r') as qw:\n",
    "    queryReader = csv.DictReader(qw)\n",
    "\n",
    "    for q in queryReader:\n",
    "        text_left = q['text_left']\n",
    "        queryInfo[text_left] = q[\"id_left\"]\n",
    "        query['bool']['should'][0]['match']['text_right'] = text_left\n",
    "        queryToDocs[text_left], partialDocInfo = search(query)\n",
    "        docInfo.update(partialDocInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b6933",
   "metadata": {},
   "source": [
    "## ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fd967ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alpha_test/ST_\" + str(alpha_best) + \".res\", \"w\") as run:\n",
    "    for qry, doc_pairs in queryToDocs.items():\n",
    "        docs = []\n",
    "        es_scores = []\n",
    "\n",
    "        for doc in doc_pairs:\n",
    "            docs.append(doc[1])\n",
    "            es_scores.append(doc[0])\n",
    "\n",
    "        query_embedding = model.encode(qry, convert_to_tensor=True)\n",
    "        doc_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "        cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "        result_scores = []\n",
    "\n",
    "        for i in range(0, 50):\n",
    "            result_score = alpha_best * es_scores[i] + (1 - alpha_best) * cos_scores[i]\n",
    "            result_scores.append(result_score)\n",
    "\n",
    "        top_results = torch.topk(torch.tensor(result_scores), k=50)\n",
    "\n",
    "        rank = 0\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            run.write(str(queryInfo[qry]) + \" Q0 \" + str(docInfo[docs[idx]]) + \" \" + str(rank) + \" \" + str(float(score)) + \" ST\\n\")\n",
    "            rank += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eecf92",
   "metadata": {},
   "source": [
    "# Precision evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1bfdcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@20: 0.154, P@10: 0.22599999999999995, AP@20: 0.17337246524327976}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('test/qrels')\n",
    "run = ir_measures.read_trec_run('alpha_test/ST_0.25'  + '.res')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP@20], qrels, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa772b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
